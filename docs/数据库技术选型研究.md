# PDFæ–‡çŒ®åˆ†ææ™ºèƒ½ä½“ - æ•°æ®åº“æŠ€æœ¯é€‰å‹ç ”ç©¶

## ğŸ“‹ æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æäº†PDFæ–‡çŒ®åˆ†ææ™ºèƒ½ä½“é¡¹ç›®ä¸­æ•°æ®åº“æŠ€æœ¯é€‰å‹çš„è€ƒè™‘å› ç´ ï¼Œè§£é‡Šäº†ä¸ºä»€ä¹ˆé€‰æ‹©PostgreSQLã€ChromaDBã€Redisè¿™ä¸‰ç§æ•°æ®åº“æŠ€æœ¯çš„ç»„åˆæ¶æ„ã€‚

**é¡¹ç›®èƒŒæ™¯**ï¼šåŸºäºAIæŠ€æœ¯çš„PDFæ–‡æ¡£æ™ºèƒ½åˆ†æå’Œé—®ç­”ç³»ç»Ÿ
**æŠ€æœ¯æ ˆ**ï¼šFastAPI + Celery + Vue.js + Docker
**æ•°æ®ç‰¹ç‚¹**ï¼šç»“æ„åŒ–ä¸šåŠ¡æ•°æ® + éç»“æ„åŒ–æ–‡æ¡£å†…å®¹ + é«˜ç»´å‘é‡æ•°æ®

---

## ğŸ—ï¸ æ•´ä½“æ•°æ®æ¶æ„è®¾è®¡

### æ•°æ®åˆ†å±‚æ¶æ„

```mermaid
graph TD
    A[ç”¨æˆ·è¯·æ±‚] --> B[FastAPIåº”ç”¨å±‚]
    B --> C[ä¸šåŠ¡æ•°æ®å±‚ - PostgreSQL]
    B --> D[å‘é‡æ•°æ®å±‚ - ChromaDB]
    B --> E[ç¼“å­˜æ•°æ®å±‚ - Redis]
    
    C --> F[æ–‡æ¡£å…ƒæ•°æ®ç®¡ç†]
    C --> G[æŸ¥è¯¢å†å²è®°å½•]
    C --> H[ç”¨æˆ·ä¼šè¯ç®¡ç†]
    
    D --> I[æ–‡æ¡£å‘é‡åµŒå…¥]
    D --> J[è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢]
    D --> K[æ–‡æ¡£å—æ£€ç´¢]
    
    E --> L[APIå“åº”ç¼“å­˜]
    E --> M[Celeryä»»åŠ¡é˜Ÿåˆ—]
    E --> N[å®æ—¶çŠ¶æ€æ•°æ®]
```

### è®¾è®¡åŸåˆ™

1. **èŒè´£åˆ†ç¦»** - ä¸åŒç±»å‹çš„æ•°æ®ä½¿ç”¨æœ€é€‚åˆçš„å­˜å‚¨æŠ€æœ¯
2. **æ€§èƒ½ä¼˜åŒ–** - é’ˆå¯¹ä¸åŒè®¿é—®æ¨¡å¼ä¼˜åŒ–å­˜å‚¨æ–¹æ¡ˆ
3. **å¯æ‰©å±•æ€§** - æ”¯æŒæ°´å¹³æ‰©å±•å’ŒåŠŸèƒ½æ‰©å±•
4. **æ•°æ®ä¸€è‡´æ€§** - ä¿è¯å…³é”®ä¸šåŠ¡æ•°æ®çš„ACIDç‰¹æ€§
5. **è¿ç»´å‹å¥½** - é€‰æ‹©æˆç†Ÿç¨³å®šçš„æŠ€æœ¯æ ˆ

---

## ğŸ—„ï¸ PostgreSQL - ä¸šåŠ¡æ•°æ®æ ¸å¿ƒ

### é€‰æ‹©ç†ç”±

#### 1. **å¼ºå¤§çš„å…³ç³»æ•°æ®å¤„ç†èƒ½åŠ›** ğŸ’ª

```sql
-- å¤æ‚çš„ä¸šåŠ¡æŸ¥è¯¢ç¤ºä¾‹
SELECT 
    d.filename,
    d.status,
    COUNT(qh.id) as query_count,
    AVG(qh.confidence) as avg_confidence,
    AVG(qh.processing_time) as avg_processing_time
FROM documents d
LEFT JOIN query_history qh ON d.id = qh.document_id
WHERE d.upload_time >= NOW() - INTERVAL '30 days'
GROUP BY d.id, d.filename, d.status
HAVING COUNT(qh.id) > 5
ORDER BY avg_confidence DESC;
```

#### 2. **åŸç”ŸJSON/JSONBæ”¯æŒ** ğŸ“Š

```python
# å­˜å‚¨å¤æ‚çš„æ–‡æ¡£å…ƒæ•°æ®
class Document(Base):
    __tablename__ = "documents"
    
    id = Column(String, primary_key=True)
    filename = Column(String, nullable=False)
    metadata = Column(JSON, nullable=True)  # çµæ´»çš„å…ƒæ•°æ®å­˜å‚¨
    
# æŸ¥è¯¢JSONå­—æ®µ
documents = session.query(Document).filter(
    Document.metadata['author'].astext == 'æŸä½œè€…'
).all()
```

#### 3. **ä¸¥æ ¼çš„ACIDäº‹åŠ¡ä¿è¯** ğŸ”’

```python
# ç¡®ä¿æ•°æ®ä¸€è‡´æ€§çš„äº‹åŠ¡å¤„ç†
async def create_document_with_history(file_info, initial_query):
    async with db.transaction():
        # 1. åˆ›å»ºæ–‡æ¡£è®°å½•
        document = await create_document(file_info)
        
        # 2. è®°å½•åˆå§‹æŸ¥è¯¢
        await create_query_history(document.id, initial_query)
        
        # 3. æ›´æ–°æ–‡æ¡£çŠ¶æ€
        await update_document_status(document.id, "processing")
    # è¦ä¹ˆå…¨éƒ¨æˆåŠŸï¼Œè¦ä¹ˆå…¨éƒ¨å›æ»š
```

#### 4. **æœªæ¥å‘é‡æœç´¢é›†æˆèƒ½åŠ›** ğŸš€

```sql
-- å®‰è£…pgvectoræ‰©å±•ï¼Œæ”¯æŒå‘é‡æ“ä½œ
CREATE EXTENSION vector;

-- åˆ›å»ºå‘é‡å­—æ®µ
ALTER TABLE document_chunks 
ADD COLUMN embedding vector(1536);

-- å‘é‡ç›¸ä¼¼åº¦æœç´¢
SELECT content, metadata, 
       1 - (embedding <=> $1) as similarity
FROM document_chunks 
ORDER BY embedding <=> $1 
LIMIT 10;
```

### æ•°æ®æ¨¡å‹è®¾è®¡

#### æ ¸å¿ƒä¸šåŠ¡è¡¨ç»“æ„

```python
class Document(Base):
    """æ–‡æ¡£åŸºç¡€ä¿¡æ¯è¡¨"""
    __tablename__ = "documents"
    
    id = Column(String, primary_key=True, index=True)        # æ–‡æ¡£å”¯ä¸€æ ‡è¯†
    filename = Column(String, nullable=False)                # åŸå§‹æ–‡ä»¶å
    file_path = Column(String, nullable=False)               # æœåŠ¡å™¨å­˜å‚¨è·¯å¾„
    file_size = Column(Integer, nullable=False)              # æ–‡ä»¶å¤§å°(å­—èŠ‚)
    pages = Column(Integer, default=0)                       # PDFé¡µæ•°
    upload_time = Column(DateTime(timezone=True), server_default=func.now())
    status = Column(String, default="pending")               # pending/processing/completed/failed
    chunk_count = Column(Integer, default=0)                 # æ–‡æ¡£åˆ‡å—æ•°é‡
    metadata = Column(JSON, nullable=True)                   # æ–‡æ¡£å…ƒæ•°æ®(ä½œè€…ã€æ ‡é¢˜ç­‰)
    error_message = Column(Text, nullable=True)              # å¤„ç†é”™è¯¯ä¿¡æ¯

class QueryHistory(Base):
    """æŸ¥è¯¢å†å²è®°å½•è¡¨"""
    __tablename__ = "query_history"
    
    id = Column(Integer, primary_key=True, index=True)
    document_id = Column(String, nullable=False, index=True) # å¤–é”®å…³è”documents.id
    question = Column(Text, nullable=False)                  # ç”¨æˆ·åŸå§‹é—®é¢˜
    answer = Column(Text, nullable=False)                    # AIç”Ÿæˆå›ç­”
    confidence = Column(Float, default=0.0)                  # å›ç­”ç½®ä¿¡åº¦[0-1]
    processing_time = Column(Float, default=0.0)             # å¤„ç†è€—æ—¶(ç§’)
    sources = Column(JSON, nullable=True)                    # ç­”æ¡ˆæ¥æºä¿¡æ¯
    query_time = Column(DateTime(timezone=True), server_default=func.now())
    user_rating = Column(Integer, nullable=True)             # ç”¨æˆ·è¯„åˆ†1-5
```

### PostgreSQL vs MySQL å¯¹æ¯”

| ç‰¹æ€§å¯¹æ¯” | PostgreSQL âœ… | MySQL âš ï¸ | é¡¹ç›®å½±å“ |
|----------|---------------|----------|----------|
| **JSONæ”¯æŒ** | åŸç”ŸJSONBï¼Œé«˜æ€§èƒ½ç´¢å¼• | 5.7+æ”¯æŒï¼Œæ€§èƒ½ä¸€èˆ¬ | æ–‡æ¡£å…ƒæ•°æ®å­˜å‚¨ä¼˜åŒ– |
| **å¤æ‚æŸ¥è¯¢** | å¼ºå¤§çš„çª—å£å‡½æ•°ã€CTE | ç›¸å¯¹è¾ƒå¼± | æ•°æ®åˆ†æå’ŒæŠ¥è¡¨ç”Ÿæˆ |
| **å¹¶å‘æ€§èƒ½** | MVCCå¤šç‰ˆæœ¬å¹¶å‘æ§åˆ¶ | é”ç²’åº¦è¾ƒå¤§ | å¤šç”¨æˆ·åŒæ—¶è®¿é—® |
| **æ•°æ®ç±»å‹** | ä¸°å¯Œ(æ•°ç»„ã€UUIDç­‰) | ç›¸å¯¹æœ‰é™ | çµæ´»çš„æ•°æ®å»ºæ¨¡ |
| **æ‰©å±•æ€§** | pgvectorã€PostGISç­‰ | æ‰©å±•æ’ä»¶è¾ƒå°‘ | æœªæ¥å‘é‡æœç´¢é›†æˆ |
| **äº‹åŠ¡éš”ç¦»** | 4ç§éš”ç¦»çº§åˆ«å®Œæ•´æ”¯æŒ | éƒ¨åˆ†æ”¯æŒ | æ•°æ®ä¸€è‡´æ€§ä¿è¯ |
| **å…¨æ–‡æœç´¢** | å†…ç½®å¼ºå¤§å…¨æ–‡æœç´¢ | åŸºç¡€æ”¯æŒ | æ–‡æ¡£å†…å®¹æ£€ç´¢ |

### æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

#### 1. **ç´¢å¼•ä¼˜åŒ–**

```sql
-- æ ¸å¿ƒæŸ¥è¯¢ç´¢å¼•
CREATE INDEX idx_documents_status ON documents(status);
CREATE INDEX idx_documents_upload_time ON documents(upload_time);
CREATE INDEX idx_query_history_document_id ON query_history(document_id);
CREATE INDEX idx_query_history_query_time ON query_history(query_time);

-- JSONå­—æ®µç´¢å¼•
CREATE INDEX idx_documents_metadata_author ON documents USING GIN ((metadata->>'author'));

-- å¤åˆç´¢å¼•
CREATE INDEX idx_query_history_composite ON query_history(document_id, query_time DESC);
```

#### 2. **åˆ†åŒºè¡¨è®¾è®¡**

```sql
-- æŒ‰æ—¶é—´åˆ†åŒºæŸ¥è¯¢å†å²è¡¨
CREATE TABLE query_history_y2024m01 PARTITION OF query_history
    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');
```

---

## ğŸ§® ChromaDB - å‘é‡æ•°æ®ä¸“å®¶

### é€‰æ‹©ç†ç”±

#### 1. **ä¸“ä¸šçš„å‘é‡å­˜å‚¨å’Œæ£€ç´¢** ğŸ¯

```python
# ChromaDBä¸“é—¨ä¸ºå‘é‡æ“ä½œä¼˜åŒ–
class VectorStoreManager:
    def __init__(self):
        self.client = chromadb.PersistentClient(path="./vector_db")
    
    def create_document_collection(self, document_id: str):
        """ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºç‹¬ç«‹çš„å‘é‡é›†åˆ"""
        collection = self.client.create_collection(
            name=f"doc_{document_id}",
            metadata={"document_id": document_id}
        )
        return collection
    
    def add_document_chunks(self, document_id: str, chunks: List[Dict]):
        """æ·»åŠ æ–‡æ¡£å—å‘é‡"""
        collection = self.client.get_collection(f"doc_{document_id}")
        
        texts = [chunk["content"] for chunk in chunks]
        embeddings = self.embedding_model.encode(texts)
        metadatas = [chunk["metadata"] for chunk in chunks]
        ids = [chunk["chunk_id"] for chunk in chunks]
        
        collection.add(
            embeddings=embeddings.tolist(),
            documents=texts,
            metadatas=metadatas,
            ids=ids
        )
```

#### 2. **é«˜æ•ˆçš„ç›¸ä¼¼åº¦æœç´¢** âš¡

```python
def search_similar_chunks(self, document_id: str, query: str, k: int = 5):
    """è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢"""
    collection = self.client.get_collection(f"doc_{document_id}")
    
    # ç”ŸæˆæŸ¥è¯¢å‘é‡
    query_embedding = self.embedding_model.encode([query])
    
    # æ‰§è¡Œå‘é‡æœç´¢
    results = collection.query(
        query_embeddings=query_embedding.tolist(),
        n_results=k,
        include=['documents', 'metadatas', 'distances']
    )
    
    # æ ¼å¼åŒ–ç»“æœ
    formatted_results = []
    for i, doc in enumerate(results['documents'][0]):
        formatted_results.append({
            "content": doc,
            "metadata": results['metadatas'][0][i],
            "similarity_score": 1 - results['distances'][0][i],  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
            "chunk_id": results['ids'][0][i]
        })
    
    return formatted_results
```

#### 3. **è½»é‡çº§éƒ¨ç½²å’Œç»´æŠ¤** ğŸ“¦

```python
# ç®€å•çš„æŒä¹…åŒ–é…ç½®
client = chromadb.PersistentClient(
    path="./vector_db",
    settings=Settings(
        chroma_db_impl="duckdb+parquet",
        persist_directory="./vector_db"
    )
)

# è‡ªåŠ¨æ•°æ®æŒä¹…åŒ–ï¼Œæ— éœ€å¤æ‚é…ç½®
```

#### 4. **LangChainç”Ÿæ€é›†æˆ** ğŸ”—

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# æ— ç¼é›†æˆLangChain
vector_store = Chroma(
    collection_name=f"doc_{document_id}",
    embedding_function=OpenAIEmbeddings(),
    client=self.client
)

# ç›´æ¥æ”¯æŒLangChainçš„æ£€ç´¢é“¾
retriever = vector_store.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"score_threshold": 0.7, "k": 5}
)
```

### ChromaDB vs å…¶ä»–å‘é‡æ•°æ®åº“å¯¹æ¯”

| å¯¹æ¯”é¡¹ | ChromaDB âœ… | Pinecone | Weaviate | Milvus |
|--------|-------------|----------|----------|---------|
| **éƒ¨ç½²å¤æ‚åº¦** | æç®€å• | äº‘æœåŠ¡ | ä¸­ç­‰ | å¤æ‚ |
| **LangChainé›†æˆ** | åŸç”Ÿæ”¯æŒ | æ”¯æŒ | æ”¯æŒ | æ”¯æŒ |
| **æœ¬åœ°åŒ–éƒ¨ç½²** | å®Œå…¨æ”¯æŒ | ä»…äº‘æœåŠ¡ | æ”¯æŒ | æ”¯æŒ |
| **å­¦ä¹ æˆæœ¬** | å¾ˆä½ | ä½ | ä¸­ç­‰ | é«˜ |
| **æ‰©å±•æ€§** | ä¸­ç­‰ | ä¼˜ç§€ | ä¼˜ç§€ | ä¼˜ç§€ |
| **æˆæœ¬** | å…è´¹ | ä»˜è´¹ | å…è´¹/ä»˜è´¹ | å…è´¹ |
| **é€‚ç”¨åœºæ™¯** | ä¸­å°å‹é¡¹ç›® | å¤§è§„æ¨¡ç”Ÿäº§ | ä¼ä¸šçº§ | å¤§è§„æ¨¡é›†ç¾¤ |

### æ•°æ®ç»„ç»‡ç»“æ„

#### æ–‡æ¡£å‘é‡åŒ–æµç¨‹

```python
class DocumentProcessor:
    def process_document(self, file_path: str) -> Dict:
        """æ–‡æ¡£å¤„ç†å®Œæ•´æµç¨‹"""
        
        # 1. æå–æ–‡æœ¬å†…å®¹
        text_content = self.extract_text_from_pdf(file_path)
        
        # 2. æ–‡æœ¬åˆ†å—
        chunks = self.split_text_into_chunks(text_content)
        
        # 3. ç”Ÿæˆå‘é‡åµŒå…¥
        embeddings = self.generate_embeddings(chunks)
        
        # 4. å­˜å‚¨åˆ°ChromaDB
        self.store_vectors(document_id, chunks, embeddings)
        
        return {
            "status": "processed",
            "chunk_count": len(chunks),
            "vector_dimensions": embeddings.shape[1]
        }

    def split_text_into_chunks(self, text: str) -> List[Dict]:
        """æ™ºèƒ½æ–‡æœ¬åˆ†å—"""
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,      # æ¯å—æœ€å¤§å­—ç¬¦æ•°
            chunk_overlap=200,    # å—ä¹‹é—´é‡å å­—ç¬¦æ•°
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ";", "ï¼›"]
        )
        
        chunks = text_splitter.split_text(text)
        
        # æ·»åŠ å…ƒæ•°æ®
        formatted_chunks = []
        for i, chunk in enumerate(chunks):
            formatted_chunks.append({
                "chunk_id": f"chunk_{i}",
                "chunk_index": i,
                "content": chunk,
                "chunk_length": len(chunk),
                "metadata": {
                    "source": "pdf_extraction",
                    "chunk_type": "text"
                }
            })
        
        return formatted_chunks
```

---

## âš¡ Redis - é«˜æ€§èƒ½ç¼“å­˜å±‚

### é€‰æ‹©ç†ç”±

#### 1. **Celeryä»»åŠ¡é˜Ÿåˆ—æ”¯æŒ** ğŸ”„

```python
# Redisä½œä¸ºCeleryçš„Brokerå’ŒBackend
from celery import Celery

celery_app = Celery(
    "document_analysis",
    broker="redis://redis:6379/0",      # ä»»åŠ¡é˜Ÿåˆ—
    backend="redis://redis:6379/0"      # ç»“æœå­˜å‚¨
)

# å¼‚æ­¥æ–‡æ¡£å¤„ç†ä»»åŠ¡
@celery_app.task(bind=True)
def process_document_task(self, document_id: str, file_path: str):
    try:
        # æ›´æ–°ä»»åŠ¡è¿›åº¦
        self.update_state(
            state="PROCESSING",
            meta={"step": "extracting_text", "progress": 20}
        )
        
        # æ‰§è¡Œæ–‡æ¡£å¤„ç†
        result = document_processor.process_document(file_path)
        
        # æ›´æ–°è¿›åº¦
        self.update_state(
            state="PROCESSING", 
            meta={"step": "creating_vectors", "progress": 60}
        )
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        vector_store_path = vector_store_manager.create_vector_store(
            document_id, result["chunks"]
        )
        
        return {
            "status": "completed",
            "progress": 100,
            "vector_store_path": vector_store_path
        }
        
    except Exception as e:
        self.update_state(
            state="FAILURE",
            meta={"error": str(e), "progress": 0}
        )
        raise
```

#### 2. **APIå“åº”ç¼“å­˜** ğŸ“ˆ

```python
class CacheManager:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.default_expire = 3600  # 1å°æ—¶
    
    def search_cache_key(self, document_id: str, query: str, k: int) -> str:
        """ç”Ÿæˆæœç´¢ç¼“å­˜é”®"""
        import hashlib
        query_hash = hashlib.md5(query.encode()).hexdigest()
        return f"search:{document_id}:{query_hash}:{k}"
    
    async def cached_search(self, document_id: str, query: str, k: int):
        """å¸¦ç¼“å­˜çš„å‘é‡æœç´¢"""
        cache_key = self.search_cache_key(document_id, query, k)
        
        # æ£€æŸ¥ç¼“å­˜
        cached_result = await self.redis.get(cache_key)
        if cached_result:
            return json.loads(cached_result)
        
        # æ‰§è¡Œæœç´¢
        results = await self.vector_store.search_similar_chunks(
            document_id, query, k
        )
        
        # ç¼“å­˜ç»“æœ
        await self.redis.setex(
            cache_key, 
            self.default_expire, 
            json.dumps(results)
        )
        
        return results
```

#### 3. **å®æ—¶çŠ¶æ€ç®¡ç†** ğŸ“Š

```python
class StatusManager:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    async def update_document_status(self, document_id: str, status: dict):
        """æ›´æ–°æ–‡æ¡£å¤„ç†çŠ¶æ€"""
        await self.redis.setex(
            f"status:{document_id}",
            300,  # 5åˆ†é’Ÿè¿‡æœŸ
            json.dumps(status)
        )
    
    async def get_document_status(self, document_id: str) -> dict:
        """è·å–æ–‡æ¡£å¤„ç†çŠ¶æ€"""
        status_data = await self.redis.get(f"status:{document_id}")
        if status_data:
            return json.loads(status_data)
        return {"status": "unknown"}
    
    async def set_user_session(self, session_id: str, user_data: dict):
        """ç”¨æˆ·ä¼šè¯ç®¡ç†"""
        await self.redis.setex(
            f"session:{session_id}",
            86400,  # 24å°æ—¶
            json.dumps(user_data)
        )
```

#### 4. **åˆ†å¸ƒå¼é”å’Œé™æµ** ğŸ”’

```python
class RedisLockManager:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    async def acquire_document_lock(self, document_id: str, timeout: int = 30):
        """è·å–æ–‡æ¡£å¤„ç†é”ï¼Œé˜²æ­¢é‡å¤å¤„ç†"""
        lock_key = f"lock:document:{document_id}"
        
        # å°è¯•è·å–é”
        lock_acquired = await self.redis.set(
            lock_key, 
            "locked", 
            nx=True,  # åªåœ¨é”®ä¸å­˜åœ¨æ—¶è®¾ç½®
            ex=timeout  # è¿‡æœŸæ—¶é—´
        )
        
        return lock_acquired
    
    async def release_document_lock(self, document_id: str):
        """é‡Šæ”¾æ–‡æ¡£å¤„ç†é”"""
        lock_key = f"lock:document:{document_id}"
        await self.redis.delete(lock_key)

class RateLimiter:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    async def check_rate_limit(self, user_id: str, limit: int = 100, window: int = 3600):
        """æ£€æŸ¥ç”¨æˆ·APIè°ƒç”¨é¢‘ç‡é™åˆ¶"""
        key = f"rate_limit:{user_id}"
        
        # ä½¿ç”¨Redisè®¡æ•°å™¨
        current_count = await self.redis.incr(key)
        
        if current_count == 1:
            # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œè®¾ç½®è¿‡æœŸæ—¶é—´
            await self.redis.expire(key, window)
        
        return current_count <= limit
```

### Redis vs å…¶ä»–ç¼“å­˜æ–¹æ¡ˆå¯¹æ¯”

| å¯¹æ¯”é¡¹ | Redis âœ… | Memcached | åº”ç”¨å†…ç¼“å­˜ | æ•°æ®åº“ç¼“å­˜ |
|--------|----------|-----------|------------|------------|
| **æ•°æ®ç±»å‹** | ä¸°å¯Œ(å­—ç¬¦ä¸²ã€åˆ—è¡¨ã€é›†åˆç­‰) | ä»…å­—ç¬¦ä¸² | å–å†³äºå®ç° | æœ‰é™ |
| **æŒä¹…åŒ–** | æ”¯æŒRDBå’ŒAOF | ä¸æ”¯æŒ | åº”ç”¨é‡å¯ä¸¢å¤± | ä¾èµ–æ•°æ®åº“ |
| **åˆ†å¸ƒå¼** | åŸç”Ÿæ”¯æŒé›†ç¾¤ | å®¢æˆ·ç«¯åˆ†ç‰‡ | å›°éš¾ | å¤æ‚ |
| **ä»»åŠ¡é˜Ÿåˆ—** | å®Œç¾æ”¯æŒ | ä¸æ”¯æŒ | éœ€è¦é¢å¤–å®ç° | æ€§èƒ½å·® |
| **äº‹åŠ¡æ”¯æŒ** | æ”¯æŒ | ä¸æ”¯æŒ | å–å†³äºå®ç° | æ”¯æŒ |
| **Luaè„šæœ¬** | æ”¯æŒ | ä¸æ”¯æŒ | N/A | éƒ¨åˆ†æ”¯æŒ |
| **è¿ç»´æˆç†Ÿåº¦** | éå¸¸æˆç†Ÿ | æˆç†Ÿ | åº”ç”¨ç»‘å®š | å¤æ‚ |

---

## ğŸ”„ ä¸‰æ•°æ®åº“ååŒå·¥ä½œæµç¨‹

### å…¸å‹ä¸šåŠ¡æµç¨‹

#### 1. **æ–‡æ¡£ä¸Šä¼ å’Œå¤„ç†**

```python
async def upload_and_process_document(file: UploadFile):
    """æ–‡æ¡£ä¸Šä¼ å¤„ç†å®Œæ•´æµç¨‹"""
    
    # 1. PostgreSQL: åˆ›å»ºæ–‡æ¡£è®°å½•
    document_id = str(uuid.uuid4())
    db_document = Document(
        id=document_id,
        filename=file.filename,
        file_path=f"uploads/{document_id}_{file.filename}",
        file_size=len(await file.read()),
        status="pending"
    )
    
    async with database.transaction():
        await database.execute(documents.insert().values(**db_document.dict()))
    
    # 2. Redis: æäº¤å¼‚æ­¥å¤„ç†ä»»åŠ¡
    task = process_document_task.delay(document_id, db_document.file_path)
    
    # 3. Redis: ç¼“å­˜ä»»åŠ¡çŠ¶æ€
    await redis_client.setex(
        f"task:{task.id}",
        3600,
        json.dumps({"document_id": document_id, "status": "pending"})
    )
    
    return {
        "document_id": document_id,
        "task_id": task.id,
        "status": "pending"
    }

async def process_document_background(document_id: str, file_path: str):
    """åå°æ–‡æ¡£å¤„ç†"""
    
    try:
        # 1. Redis: æ›´æ–°å¤„ç†çŠ¶æ€
        await redis_client.setex(
            f"status:{document_id}",
            300,
            json.dumps({"status": "processing", "step": "text_extraction"})
        )
        
        # 2. å¤„ç†æ–‡æ¡£å†…å®¹
        processor = DocumentProcessor()
        result = processor.process_document(file_path)
        
        # 3. ChromaDB: å­˜å‚¨å‘é‡æ•°æ®
        vector_manager = VectorStoreManager()
        await vector_manager.create_document_collection(document_id)
        await vector_manager.add_document_chunks(document_id, result["chunks"])
        
        # 4. PostgreSQL: æ›´æ–°æ–‡æ¡£çŠ¶æ€
        await database.execute(
            documents.update()
            .where(documents.c.id == document_id)
            .values(
                status="completed",
                chunk_count=len(result["chunks"]),
                metadata=result["metadata"]
            )
        )
        
        # 5. Redis: æ¸…ç†ä¸´æ—¶çŠ¶æ€
        await redis_client.delete(f"status:{document_id}")
        
    except Exception as e:
        # é”™è¯¯å¤„ç†
        await database.execute(
            documents.update()
            .where(documents.c.id == document_id)
            .values(status="failed", error_message=str(e))
        )
```

#### 2. **æ™ºèƒ½é—®ç­”æŸ¥è¯¢**

```python
async def answer_question(document_id: str, question: str):
    """æ™ºèƒ½é—®ç­”å®Œæ•´æµç¨‹"""
    
    # 1. PostgreSQL: éªŒè¯æ–‡æ¡£çŠ¶æ€
    document = await database.fetch_one(
        documents.select().where(documents.c.id == document_id)
    )
    
    if not document or document.status != "completed":
        raise HTTPException(404, "æ–‡æ¡£æœªæ‰¾åˆ°æˆ–å¤„ç†æœªå®Œæˆ")
    
    # 2. Redis: æ£€æŸ¥æŸ¥è¯¢ç¼“å­˜
    cache_key = f"query:{document_id}:{hashlib.md5(question.encode()).hexdigest()}"
    cached_result = await redis_client.get(cache_key)
    
    if cached_result:
        # ç¼“å­˜å‘½ä¸­ï¼Œç›´æ¥è¿”å›
        result = json.loads(cached_result)
        
        # PostgreSQL: è®°å½•æŸ¥è¯¢å†å²ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
        await database.execute(
            query_history.insert().values(
                document_id=document_id,
                question=question,
                answer=result["answer"],
                confidence=result["confidence"],
                processing_time=0.001,  # ç¼“å­˜å“åº”æå¿«
                sources=result["sources"]
            )
        )
        
        return result
    
    # 3. ChromaDB: å‘é‡ç›¸ä¼¼åº¦æœç´¢
    start_time = time.time()
    
    vector_manager = VectorStoreManager()
    search_results = await vector_manager.search_similar_chunks(
        document_id=document_id,
        query=question,
        k=5
    )
    
    # 4. AIæ¨¡å‹: ç”Ÿæˆå›ç­”
    agent = DocumentAnalysisAgent()
    answer_result = await agent.answer_question(
        document_id=document_id,
        question=question,
        search_results=search_results
    )
    
    processing_time = time.time() - start_time
    
    # 5. PostgreSQL: ä¿å­˜æŸ¥è¯¢å†å²
    await database.execute(
        query_history.insert().values(
            document_id=document_id,
            question=question,
            answer=answer_result["answer"],
            confidence=answer_result["confidence"],
            processing_time=processing_time,
            sources=answer_result["sources"]
        )
    )
    
    # 6. Redis: ç¼“å­˜æŸ¥è¯¢ç»“æœ
    await redis_client.setex(
        cache_key,
        3600,  # 1å°æ—¶ç¼“å­˜
        json.dumps(answer_result)
    )
    
    return answer_result
```

### æ•°æ®ä¸€è‡´æ€§ä¿è¯

#### 1. **äº‹åŠ¡è¾¹ç•Œè®¾è®¡**

```python
class DataConsistencyManager:
    def __init__(self, postgres_db, chroma_client, redis_client):
        self.postgres = postgres_db
        self.chroma = chroma_client
        self.redis = redis_client
    
    async def safe_document_deletion(self, document_id: str):
        """å®‰å…¨åˆ é™¤æ–‡æ¡£ï¼ˆä¿è¯æ•°æ®ä¸€è‡´æ€§ï¼‰"""
        
        try:
            # 1. PostgreSQLäº‹åŠ¡å¼€å§‹
            async with self.postgres.transaction():
                # è·å–æ–‡æ¡£ä¿¡æ¯
                document = await self.postgres.fetch_one(
                    documents.select().where(documents.c.id == document_id)
                )
                
                if not document:
                    raise ValueError("æ–‡æ¡£ä¸å­˜åœ¨")
                
                # åˆ é™¤ç›¸å…³çš„æŸ¥è¯¢å†å²
                await self.postgres.execute(
                    query_history.delete().where(
                        query_history.c.document_id == document_id
                    )
                )
                
                # åˆ é™¤æ–‡æ¡£è®°å½•
                await self.postgres.execute(
                    documents.delete().where(documents.c.id == document_id)
                )
                
                # PostgreSQLäº‹åŠ¡æäº¤ç‚¹
            
            # 2. ChromaDB: åˆ é™¤å‘é‡æ•°æ®
            try:
                collection_name = f"doc_{document_id}"
                self.chroma.delete_collection(name=collection_name)
            except Exception as e:
                logger.warning(f"ChromaDBåˆ é™¤å¤±è´¥ï¼Œä½†ä¸å½±å“ä¸šåŠ¡: {e}")
            
            # 3. Redis: æ¸…ç†ç›¸å…³ç¼“å­˜
            cache_keys = await self.redis.keys(f"*{document_id}*")
            if cache_keys:
                await self.redis.delete(*cache_keys)
            
            # 4. æ–‡ä»¶ç³»ç»Ÿ: åˆ é™¤æ–‡ä»¶
            if document.file_path and os.path.exists(document.file_path):
                os.remove(document.file_path)
            
            return {"status": "success", "message": "æ–‡æ¡£åˆ é™¤æˆåŠŸ"}
            
        except Exception as e:
            # å¦‚æœPostgreSQLäº‹åŠ¡å¤±è´¥ï¼Œæ•´ä¸ªæ“ä½œéƒ½ä¼šå›æ»š
            logger.error(f"æ–‡æ¡£åˆ é™¤å¤±è´¥: {e}")
            raise
```

#### 2. **æ•°æ®åŒæ­¥ç­–ç•¥**

```python
class DataSyncManager:
    """æ•°æ®åŒæ­¥ç®¡ç†å™¨"""
    
    async def sync_document_status(self, document_id: str):
        """åŒæ­¥æ–‡æ¡£çŠ¶æ€åˆ°å„ä¸ªå­˜å‚¨"""
        
        # ä»PostgreSQLè·å–æƒå¨çŠ¶æ€
        document = await self.postgres.fetch_one(
            documents.select().where(documents.c.id == document_id)
        )
        
        if not document:
            return
        
        # åŒæ­¥åˆ°Redisç¼“å­˜
        await self.redis.setex(
            f"doc_status:{document_id}",
            3600,
            json.dumps({
                "status": document.status,
                "chunk_count": document.chunk_count,
                "last_updated": document.updated_at.isoformat()
            })
        )
        
        # éªŒè¯ChromaDBæ•°æ®ä¸€è‡´æ€§
        if document.status == "completed":
            try:
                collection = self.chroma.get_collection(f"doc_{document_id}")
                actual_count = collection.count()
                
                if actual_count != document.chunk_count:
                    logger.warning(
                        f"æ•°æ®ä¸ä¸€è‡´: PostgreSQLæ˜¾ç¤º{document.chunk_count}å—ï¼Œ"
                        f"ä½†ChromaDBæœ‰{actual_count}å—"
                    )
                    
                    # è§¦å‘æ•°æ®ä¿®å¤
                    await self.trigger_data_repair(document_id)
                    
            except Exception as e:
                logger.error(f"ChromaDBä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {e}")
    
    async def trigger_data_repair(self, document_id: str):
        """è§¦å‘æ•°æ®ä¿®å¤ä»»åŠ¡"""
        repair_task.delay(document_id)
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. **æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–**

#### PostgreSQLä¼˜åŒ–

```sql
-- åˆ›å»ºè¦†ç›–ç´¢å¼•
CREATE INDEX idx_query_history_performance 
ON query_history(document_id, query_time DESC) 
INCLUDE (question, confidence, processing_time);

-- åˆ†åŒºè¡¨ä¼˜åŒ–å¤§æ•°æ®é‡
CREATE TABLE query_history_partitioned (
    LIKE query_history INCLUDING ALL
) PARTITION BY RANGE (query_time);

-- ç‰©åŒ–è§†å›¾åŠ é€Ÿç»Ÿè®¡æŸ¥è¯¢
CREATE MATERIALIZED VIEW document_stats AS
SELECT 
    d.id,
    d.filename,
    d.status,
    COUNT(qh.id) as total_queries,
    AVG(qh.confidence) as avg_confidence,
    AVG(qh.processing_time) as avg_processing_time,
    MAX(qh.query_time) as last_query_time
FROM documents d
LEFT JOIN query_history qh ON d.id = qh.document_id
GROUP BY d.id, d.filename, d.status;

-- å®šæœŸåˆ·æ–°ç‰©åŒ–è§†å›¾
REFRESH MATERIALIZED VIEW CONCURRENTLY document_stats;
```

#### ChromaDBä¼˜åŒ–

```python
# æ‰¹é‡æ“ä½œä¼˜åŒ–
class OptimizedVectorStore:
    def __init__(self):
        self.batch_size = 100
        self.client = chromadb.PersistentClient(
            path="./vector_db",
            settings=Settings(
                # ä¼˜åŒ–è®¾ç½®
                chroma_db_impl="duckdb+parquet",
                persist_directory="./vector_db",
                anonymized_telemetry=False
            )
        )
    
    async def batch_add_chunks(self, document_id: str, chunks: List[Dict]):
        """æ‰¹é‡æ·»åŠ æ–‡æ¡£å—"""
        collection = self.client.get_collection(f"doc_{document_id}")
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(chunks), self.batch_size):
            batch = chunks[i:i + self.batch_size]
            
            texts = [chunk["content"] for chunk in batch]
            embeddings = await self.embedding_model.aencode(texts)
            metadatas = [chunk["metadata"] for chunk in batch]
            ids = [chunk["chunk_id"] for chunk in batch]
            
            collection.add(
                embeddings=embeddings.tolist(),
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )
```

#### Redisä¼˜åŒ–

```python
# è¿æ¥æ± å’Œç®¡é“ä¼˜åŒ–
class OptimizedRedisManager:
    def __init__(self):
        self.pool = redis.ConnectionPool(
            host='redis',
            port=6379,
            db=0,
            max_connections=20,
            decode_responses=True
        )
        self.redis = redis.Redis(connection_pool=self.pool)
    
    async def batch_cache_operations(self, operations: List[tuple]):
        """æ‰¹é‡ç¼“å­˜æ“ä½œ"""
        pipe = self.redis.pipeline()
        
        for op_type, key, value, expire in operations:
            if op_type == "set":
                pipe.setex(key, expire, json.dumps(value))
            elif op_type == "get":
                pipe.get(key)
        
        results = pipe.execute()
        return results
    
    async def smart_cache_strategy(self, key: str, value: any, 
                                 access_pattern: str = "normal"):
        """æ™ºèƒ½ç¼“å­˜ç­–ç•¥"""
        if access_pattern == "hot":
            expire = 3600 * 6  # 6å°æ—¶
        elif access_pattern == "warm": 
            expire = 3600 * 2  # 2å°æ—¶
        else:
            expire = 3600      # 1å°æ—¶
        
        await self.redis.setex(key, expire, json.dumps(value))
```

### 2. **å†…å­˜ä½¿ç”¨ä¼˜åŒ–**

```python
class MemoryOptimizedSystem:
    def __init__(self):
        self.max_cache_size = 1000  # æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
        self.cleanup_threshold = 0.8  # æ¸…ç†é˜ˆå€¼
    
    async def memory_aware_caching(self, key: str, value: any):
        """å†…å­˜æ„ŸçŸ¥çš„ç¼“å­˜ç­–ç•¥"""
        current_memory = psutil.virtual_memory().percent
        
        if current_memory > 80:
            # å†…å­˜ç´§å¼ ï¼Œä½¿ç”¨è¾ƒçŸ­çš„è¿‡æœŸæ—¶é—´
            expire = 300  # 5åˆ†é’Ÿ
        elif current_memory > 60:
            # å†…å­˜é€‚ä¸­
            expire = 1800  # 30åˆ†é’Ÿ
        else:
            # å†…å­˜å……è¶³
            expire = 3600  # 1å°æ—¶
        
        await self.redis.setex(key, expire, json.dumps(value))
    
    async def cleanup_expired_data(self):
        """æ¸…ç†è¿‡æœŸæ•°æ®"""
        # æ¸…ç†PostgreSQLä¸­çš„æ—§æŸ¥è¯¢è®°å½•
        await self.postgres.execute("""
            DELETE FROM query_history 
            WHERE query_time < NOW() - INTERVAL '30 days'
        """)
        
        # æ¸…ç†Redisè¿‡æœŸé”®
        await self.redis.execute_command("MEMORY", "PURGE")
```

---

## ğŸ”§ è¿ç»´å’Œç›‘æ§

### 1. **å¥åº·æ£€æŸ¥**

```python
class HealthCheckManager:
    async def check_all_services(self) -> Dict[str, str]:
        """æ£€æŸ¥æ‰€æœ‰æ•°æ®åº“æœåŠ¡å¥åº·çŠ¶æ€"""
        results = {}
        
        # PostgreSQLå¥åº·æ£€æŸ¥
        try:
            await self.postgres.fetch_one("SELECT 1")
            results["postgresql"] = "healthy"
        except Exception as e:
            results["postgresql"] = f"unhealthy: {str(e)}"
        
        # ChromaDBå¥åº·æ£€æŸ¥
        try:
            collections = self.chroma.list_collections()
            results["chromadb"] = f"healthy ({len(collections)} collections)"
        except Exception as e:
            results["chromadb"] = f"unhealthy: {str(e)}"
        
        # Rediså¥åº·æ£€æŸ¥
        try:
            await self.redis.ping()
            info = await self.redis.info()
            results["redis"] = f"healthy (memory: {info['used_memory_human']})"
        except Exception as e:
            results["redis"] = f"unhealthy: {str(e)}"
        
        return results
```

### 2. **æ€§èƒ½ç›‘æ§**

```python
class PerformanceMonitor:
    def __init__(self):
        self.metrics = {}
    
    async def collect_metrics(self):
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        
        # PostgreSQLæŒ‡æ ‡
        pg_stats = await self.postgres.fetch_one("""
            SELECT 
                (SELECT count(*) FROM documents) as total_documents,
                (SELECT count(*) FROM query_history) as total_queries,
                (SELECT avg(processing_time) FROM query_history 
                 WHERE query_time > NOW() - INTERVAL '1 hour') as avg_processing_time
        """)
        
        # RedisæŒ‡æ ‡
        redis_info = await self.redis.info()
        
        # ChromaDBæŒ‡æ ‡
        collections = self.chroma.list_collections()
        total_vectors = sum(col.count() for col in collections)
        
        metrics = {
            "postgresql": {
                "total_documents": pg_stats["total_documents"],
                "total_queries": pg_stats["total_queries"],
                "avg_processing_time": float(pg_stats["avg_processing_time"] or 0)
            },
            "redis": {
                "memory_usage": redis_info["used_memory"],
                "connected_clients": redis_info["connected_clients"],
                "keyspace_hits": redis_info["keyspace_hits"],
                "keyspace_misses": redis_info["keyspace_misses"]
            },
            "chromadb": {
                "total_collections": len(collections),
                "total_vectors": total_vectors
            }
        }
        
        # å­˜å‚¨åˆ°æ—¶åºæ•°æ®åº“æˆ–æ—¥å¿—
        await self.store_metrics(metrics)
        
        return metrics
```

### 3. **å¤‡ä»½ç­–ç•¥**

```bash
#!/bin/bash
# æ•°æ®å¤‡ä»½è„šæœ¬

# PostgreSQLå¤‡ä»½
pg_dump -h postgres -U postgres document_analysis > /backup/postgres_$(date +%Y%m%d_%H%M%S).sql

# Rediså¤‡ä»½
redis-cli --rdb /backup/redis_$(date +%Y%m%d_%H%M%S).rdb

# ChromaDBå¤‡ä»½
tar -czf /backup/chromadb_$(date +%Y%m%d_%H%M%S).tar.gz ./vector_db/

# ä¸Šä¼ æ–‡ä»¶å¤‡ä»½
tar -czf /backup/uploads_$(date +%Y%m%d_%H%M%S).tar.gz ./uploads/

# æ¸…ç†7å¤©å‰çš„å¤‡ä»½
find /backup -name "*.sql" -mtime +7 -delete
find /backup -name "*.rdb" -mtime +7 -delete
find /backup -name "*.tar.gz" -mtime +7 -delete
```

---

## ğŸš€ æœªæ¥æ¼”è¿›è·¯çº¿

### 1. **çŸ­æœŸä¼˜åŒ–ï¼ˆ1-3ä¸ªæœˆï¼‰**

- **PostgreSQL pgvectoré›†æˆ**ï¼šé€æ­¥å°†å‘é‡æœç´¢è¿ç§»åˆ°PostgreSQL
- **Redis Clusteréƒ¨ç½²**ï¼šæ”¯æŒæ°´å¹³æ‰©å±•å’Œé«˜å¯ç”¨
- **ç›‘æ§ä»ªè¡¨æ¿**ï¼šGrafana + Prometheusç›‘æ§å¤§ç›˜

### 2. **ä¸­æœŸæ‰©å±•ï¼ˆ3-6ä¸ªæœˆï¼‰**

- **è¯»å†™åˆ†ç¦»**ï¼šPostgreSQLä¸»ä»æ¶æ„
- **ç¼“å­˜ç­–ç•¥ä¼˜åŒ–**ï¼šå¤šçº§ç¼“å­˜ä½“ç³»
- **åˆ†å¸ƒå¼å‘é‡æœç´¢**ï¼šæ”¯æŒæ›´å¤§è§„æ¨¡æ–‡æ¡£åº“

### 3. **é•¿æœŸè§„åˆ’ï¼ˆ6-12ä¸ªæœˆï¼‰**

- **å¤šæ¨¡æ€æ”¯æŒ**ï¼šå›¾ç‰‡ã€è¡¨æ ¼å‘é‡åŒ–
- **çŸ¥è¯†å›¾è°±é›†æˆ**ï¼šNeo4jå­˜å‚¨å®ä½“å…³ç³»
- **å®æ—¶æ•°æ®æµ**ï¼šKafka + ClickHouseåˆ†æ

---

## ğŸ“‹ æ€»ç»“

### æŠ€æœ¯é€‰å‹å†³ç­–æ€»ç»“

| æ•°æ®åº“ | ä¸»è¦èŒè´£ | é€‰æ‹©ç†ç”± | æ›¿ä»£æ–¹æ¡ˆå¯¹æ¯” |
|--------|----------|----------|-------------|
| **PostgreSQL** | ä¸šåŠ¡æ•°æ®ç®¡ç† | ACIDäº‹åŠ¡ã€JSONæ”¯æŒã€æœªæ¥pgvectoré›†æˆ | MySQLæ€§èƒ½ç•¥é€Šï¼ŒåŠŸèƒ½ä¸å¤Ÿä¸°å¯Œ |
| **ChromaDB** | å‘é‡å­˜å‚¨æ£€ç´¢ | LangChainé›†æˆã€éƒ¨ç½²ç®€å•ã€å­¦ä¹ æˆæœ¬ä½ | Pineconeå¤ªè´µï¼ŒMilvuså¤ªå¤æ‚ |
| **Redis** | ç¼“å­˜å’Œé˜Ÿåˆ— | Celeryæ”¯æŒã€é«˜æ€§èƒ½ã€ç”Ÿæ€æˆç†Ÿ | MemcachedåŠŸèƒ½æœ‰é™ï¼ŒRabbitMQå­¦ä¹ æˆæœ¬é«˜ |

### æ¶æ„ä¼˜åŠ¿

1. **èŒè´£æ¸…æ™°**ï¼šæ¯ç§æ•°æ®åº“ä¸“æ³¨äºè‡ªå·±æ“…é•¿çš„é¢†åŸŸ
2. **æ€§èƒ½ä¼˜åŒ–**ï¼šé’ˆå¯¹ä¸åŒè®¿é—®æ¨¡å¼ä¼˜åŒ–å­˜å‚¨ç­–ç•¥
3. **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒæ°´å¹³æ‰©å±•å’ŒåŠŸèƒ½æ‰©å±•
4. **è¿ç»´å‹å¥½**ï¼šæˆç†Ÿçš„å·¥å…·é“¾å’Œæœ€ä½³å®è·µ
5. **æˆæœ¬å¯æ§**ï¼šå¼€æºæ–¹æ¡ˆï¼Œé¿å…ä¾›åº”å•†é”å®š

### å…³é”®æˆåŠŸå› ç´ 

1. **æ•°æ®ä¸€è‡´æ€§**ï¼šé€šè¿‡äº‹åŠ¡å’Œè¡¥å¿æœºåˆ¶ä¿è¯
2. **æ€§èƒ½ç›‘æ§**ï¼šå®æ—¶ç›‘æ§å’Œé¢„è­¦æœºåˆ¶
3. **æ•…éšœæ¢å¤**ï¼šå®Œå–„çš„å¤‡ä»½å’Œæ¢å¤ç­–ç•¥
4. **å›¢é˜Ÿèƒ½åŠ›**ï¼šæŠ€æœ¯æ ˆç¬¦åˆå›¢é˜ŸæŠ€èƒ½æ°´å¹³



è¿™ç§ä¸‰æ•°æ®åº“ååŒçš„æ¶æ„è®¾è®¡ï¼Œæ—¢æ»¡è¶³äº†å½“å‰çš„ä¸šåŠ¡éœ€æ±‚ï¼Œåˆä¸ºæœªæ¥çš„åŠŸèƒ½æ‰©å±•é¢„ç•™äº†å……è¶³çš„ç©ºé—´ï¼Œæ˜¯ä¸€ä¸ªç»è¿‡æ·±æ€ç†Ÿè™‘çš„æŠ€æœ¯é€‰å‹å†³ç­–ã€‚
