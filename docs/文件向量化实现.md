# 基于LangChain自构建PDF文献分析智能体实现方案

## 1. 系统架构设计

用户界面 (Vue.js/React)
↓
API服务 (FastAPI)
↓
文档处理服务 (Celery Worker)
↓
智能体核心 (LangChain)
↓
存储层 (向量数据库 + PostgreSQL)


## 2. 技术栈选择

- **后端框架**: FastAPI (异步高性能)
- **任务队列**: Celery (处理长时间运行的PDF处理任务)
- **向量数据库**: Chroma (轻量级、易于部署)
- **关系数据库**: PostgreSQL (存储元数据和用户信息)
- **LLM接口**: OpenAI API (可替换为其他模型)
- **前端**: Vue.js (可选，也可使用React)
- **部署**: Docker Compose (简化部署流程)

## 3. 核心组件实现

### 3.1 文档处理器 (document_processor.py)

```python
import os
import uuid
import fitz  # PyMuPDF
from typing import Dict, List, Any
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

class DocumentProcessor:
    """处理PDF文档，提取文本并进行分块"""
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
        )
    
    def extract_text_from_pdf(self, file_path: str) -> str:
        """从PDF文件提取文本"""
        try:
            text = ""
            with fitz.open(file_path) as doc:
                metadata = {
                    "title": doc.metadata.get("title", ""),
                    "author": doc.metadata.get("author", ""),
                    "subject": doc.metadata.get("subject", ""),
                    "total_pages": len(doc)
                }
                
                # 提取每一页的文本
                for page_num, page in enumerate(doc):
                    text += f"\n\n--- Page {page_num + 1} ---\n\n"
                    text += page.get_text()
            
            return text, metadata
        except Exception as e:
            raise Exception(f"PDF文本提取失败: {str(e)}")
    
    def split_text(self, text: str, metadata: Dict[str, Any]) -> List[Document]:
        """将文本分割成块"""
        try:
            docs = self.text_splitter.create_documents([text], [metadata])
            
            # 添加页码和块ID信息
            for i, doc in enumerate(docs):
                doc.metadata["chunk_id"] = i
                # 尝试从文本中提取页码
                page_match = re.search(r"--- Page (\d+) ---", doc.page_content)
                if page_match:
                    doc.metadata["page"] = int(page_match.group(1))
                else:
                    doc.metadata["page"] = "未知"
                    
            return docs
        except Exception as e:
            raise Exception(f"文本分块失败: {str(e)}")
    
    def process_document(self, file_path: str) -> Dict[str, Any]:
        """处理文档的完整流程"""
        document_id = str(uuid.uuid4())
        
        try:
            # 提取文本
            text, metadata = self.extract_text_from_pdf(file_path)
            
            # 分割文本
            docs = self.split_text(text, metadata)
            
            return {
                "document_id": document_id,
                "chunks": docs,
                "metadata": metadata,
                "status": "processed",
                "chunk_count": len(docs)
            }
        except Exception as e:
            return {
                "document_id": document_id,
                "status": "failed",
                "error": str(e)
            }
```

### 3.2 向量存储管理器 (vector_store.py)

```python
import os
from typing import List, Dict, Any, Optional
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.schema import Document

class VectorStoreManager:
    """管理向量存储的创建、查询和维护"""
    
    def __init__(self, embedding_model=None, persist_directory="./vector_db"):
        self.persist_directory = persist_directory
        self.embedding_model = embedding_model or OpenAIEmbeddings()
        
        # 确保存储目录存在
        os.makedirs(persist_directory, exist_ok=True)
    
    def create_vector_store(self, document_id: str, chunks: List[Document]) -> str:
        """为文档创建向量存储"""
        try:
            # 为每个文档创建单独的向量存储目录
            doc_persist_dir = os.path.join(self.persist_directory, document_id)
            os.makedirs(doc_persist_dir, exist_ok=True)
            
            # 创建向量存储
            vector_store = Chroma.from_documents(
                documents=chunks,
                embedding=self.embedding_model,
                persist_directory=doc_persist_dir
            )
            
            # 持久化存储
            vector_store.persist()
            
            return doc_persist_dir
        except Exception as e:
            raise Exception(f"创建向量存储失败: {str(e)}")
    
    def load_vector_store(self, document_id: str) -> Chroma:
        """加载文档的向量存储"""
        doc_persist_dir = os.path.join(self.persist_directory, document_id)
        
        if not os.path.exists(doc_persist_dir):
            raise Exception(f"向量存储不存在: {document_id}")
        
        return Chroma(
            persist_directory=doc_persist_dir,
            embedding_function=self.embedding_model
        )
    
    def search_documents(self, document_id: str, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """在向量存储中搜索相关文档"""
        try:
            vector_store = self.load_vector_store(document_id)
            results = vector_store.similarity_search_with_score(query, k=top_k)
            
            formatted_results = []
            for doc, score in results:
                formatted_results.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "relevance_score": float(score)
                })
            
            return formatted_results
        except Exception as e:
            raise Exception(f"文档搜索失败: {str(e)}")
    
    def delete_vector_store(self, document_id: str) -> bool:
        """删除文档的向量存储"""
        doc_persist_dir = os.path.join(self.persist_directory, document_id)
        
        if os.path.exists(doc_persist_dir):
            import shutil
            shutil.rmtree(doc_persist_dir)
            return True
        
        return False
```

### 3.3 智能体核心 (agent_core.py)

```python
from typing import Dict, List, Any, Optional
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate
from langchain.schema import SystemMessage, HumanMessage

class DocumentAnalysisAgent:
    """文献分析智能体，负责回答基于文档的问题"""
    
    def __init__(self, llm=None, vector_store_manager=None):
        self.llm = llm or ChatOpenAI(temperature=0)
        self.vector_store_manager = vector_store_manager
        
        # 初始化提示模板
        self.qa_prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(
                """你是一个专业的文献分析助手。你的任务是根据提供的文献内容回答用户的问题。
                请只使用提供的文献内容作为回答的依据。如果文献内容中没有相关信息，请明确说明。
                请以学术严谨的方式回答，必要时引用原文内容。"""
            ),
            HumanMessagePromptTemplate.from_template(
                """根据以下文献内容回答问题:
                
                {context}
                
                问题: {question}
                """
            )
        ])
    
    def answer_question(self, document_id: str, question: str, top_k: int = 5) -> Dict[str, Any]:
        """回答基于文档的问题"""
        try:
            # 从向量存储中检索相关内容
            search_results = self.vector_store_manager.search_documents(
                document_id=document_id,
                query=question,
                top_k=top_k
            )
            
            if not search_results:
                return {
                    "answer": "抱歉，我在文档中找不到相关信息来回答这个问题。",
                    "sources": []
                }
            
            # 构建上下文
            context = "\n\n".join([
                f"文段 {i+1} (相关度: {result['relevance_score']:.2f}, 页码: {result['metadata'].get('page', '未知')}):\n{result['content']}"
                for i, result in enumerate(search_results)
            ])
            
            # 创建LLM链并生成回答
            qa_chain = LLMChain(llm=self.llm, prompt=self.qa_prompt)
            response = qa_chain.run(context=context, question=question)
            
            # 格式化来源信息
            sources = []
            for result in search_results:
                sources.append({
                    "page": result["metadata"].get("page", "未知"),
                    "chunk_id": result["metadata"].get("chunk_id", ""),
                    "relevance": result["relevance_score"],
                    "preview": result["content"][:150] + "..." if len(result["content"]) > 150 else result["content"]
                })
            
            return {
                "answer": response,
                "sources": sources
            }
        except Exception as e:
            return {
                "error": f"回答问题时出错: {str(e)}",
                "answer": "抱歉，处理您的问题时出现技术问题。",
                "sources": []
            }
```

### 3.4 数据模型 (models.py)

```python
from datetime import datetime
from typing import Optional, List, Dict, Any
from pydantic import BaseModel, Field

class DocumentMetadata(BaseModel):
    """文档元数据"""
    title: str = ""
    author: str = ""
    subject: str = ""
    total_pages: int = 0
    file_size: int = 0
    file_type: str = "pdf"
    created_at: datetime = Field(default_factory=datetime.now)

class Document(BaseModel):
    """文档模型"""
    id: str
    filename: str
    status: str  # 'processing', 'processed', 'failed'
    metadata: DocumentMetadata
    vector_store_path: Optional[str] = None
    chunk_count: Optional[int] = None
    error: Optional[str] = None
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)

class SearchResult(BaseModel):
    """搜索结果"""
    content: str
    metadata: Dict[str, Any]
    relevance_score: float

class QuestionRequest(BaseModel):
    """问题请求"""
    document_id: str
    question: str
    top_k: int = 5

class QuestionResponse(BaseModel):
    """问题回答"""
    answer: str
    sources: List[Dict[str, Any]] = []
    error: Optional[str] = None
```

### 3.5 API服务 (app.py)

```python
import os
import uuid
import shutil
from datetime import datetime
from typing import Dict, Any, List

from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from sqlalchemy.orm import Session

from .database import get_db, engine
from . import models, schemas
from .document_processor import DocumentProcessor
from .vector_store import VectorStoreManager
from .agent_core import DocumentAnalysisAgent

# 创建数据库表
models.Base.metadata.create_all(bind=engine)

app = FastAPI(title="文献分析智能体API")

# 配置CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 初始化组件
document_processor = DocumentProcessor()
vector_store_manager = VectorStoreManager()
agent = DocumentAnalysisAgent(vector_store_manager=vector_store_manager)

# 上传目录
UPLOAD_DIR = "./uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)

@app.post("/documents/upload", response_model=schemas.DocumentResponse)
async def upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    db: Session = Depends(get_db)
):
    """上传PDF文档并异步处理"""
    # 验证文件类型
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(status_code=400, detail="只支持PDF文件")
    
    # 生成唯一文件名
    document_id = str(uuid.uuid4())
    file_path = os.path.join(UPLOAD_DIR, f"{document_id}.pdf")
    
    # 保存上传文件
    try:
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"文件保存失败: {str(e)}")
    
    # 创建文档记录
    new_document = models.Document(
        id=document_id,
        filename=file.filename,
        status="processing",
        file_path=file_path,
        created_at=datetime.now(),
        updated_at=datetime.now()
    )
    db.add(new_document)
    db.commit()
    
    # 后台处理文档
    background_tasks.add_task(process_document_task, document_id, file_path, db)
    
    return {
        "document_id": document_id,
        "filename": file.filename,
        "status": "processing",
        "message": "文档已上传并开始处理"
    }

async def process_document_task(document_id: str, file_path: str, db: Session):
    """后台处理文档任务"""
    try:
        # 处理文档
        result = document_processor.process_document(file_path)
        
        if result["status"] == "failed":
            # 更新失败状态
            db.query(models.Document).filter(models.Document.id == document_id).update({
                "status": "failed",
                "error": result["error"],
                "updated_at": datetime.now()
            })
            db.commit()
            return
        
        # 创建向量存储
        vector_store_path = vector_store_manager.create_vector_store(
            document_id=document_id,
            chunks=result["chunks"]
        )
        
        # 更新文档状态
        db.query(models.Document).filter(models.Document.id == document_id).update({
            "status": "processed",
            "metadata": result["metadata"],
            "vector_store_path": vector_store_path,
            "chunk_count": result["chunk_count"],
            "updated_at": datetime.now()
        })
        db.commit()
        
    except Exception as e:
        # 更新失败状态
        db.query(models.Document).filter(models.Document.id == document_id).update({
            "status": "failed",
            "error": str(e),
            "updated_at": datetime.now()
        })
        db.commit()

@app.get("/documents/{document_id}", response_model=schemas.DocumentDetail)
async def get_document_status(document_id: str, db: Session = Depends(get_db)):
    """获取文档处理状态"""
    document = db.query(models.Document).filter(models.Document.id == document_id).first()
    
    if not document:
        raise HTTPException(status_code=404, detail="文档不存在")
    
    return document

@app.post("/documents/{document_id}/query", response_model=schemas.QuestionResponse)
async def query_document(
    document_id: str,
    request: schemas.QuestionRequest,
    db: Session = Depends(get_db)
):
    """查询文档并回答问题"""
    # 检查文档是否存在
    document = db.query(models.Document).filter(models.Document.id == document_id).first()
    
    if not document:
        raise HTTPException(status_code=404, detail="文档不存在")
    
    if document.status != "processed":
        raise HTTPException(status_code=400, detail=f"文档尚未处理完成，当前状态: {document.status}")
    
    # 回答问题
    response = agent.answer_question(
        document_id=document_id,
        question=request.question,
        top_k=request.top_k
    )
    
    return response

@app.get("/documents", response_model=List[schemas.DocumentListItem])
async def list_documents(db: Session = Depends(get_db)):
    """列出所有文档"""
    documents = db.query(models.Document).order_by(models.Document.created_at.desc()).all()
    return documents

@app.delete("/documents/{document_id}", response_model=schemas.DocumentResponse)
async def delete_document(document_id: str, db: Session = Depends(get_db)):
    """删除文档及其向量存储"""
    document = db.query(models.Document).filter(models.Document.id == document_id).first()
    
    if not document:
        raise HTTPException(status_code=404, detail="文档不存在")
    
    # 删除向量存储
    if document.vector_store_path:
        vector_store_manager.delete_vector_store(document_id)
    
    # 删除文件
    if document.file_path and os.path.exists(document.file_path):
        os.remove(document.file_path)
    
    # 删除数据库记录
    db.delete(document)
    db.commit()
    
    return {
        "document_id": document_id,
        "status": "deleted",
        "message": "文档已成功删除"
    }
```

## 4. 前端界面 (Vue.js)

### 4.1 文档上传组件 (DocumentUpload.vue)

```vue
<template>
  <div class="document-upload">
    <h2>上传文献</h2>
    
    <div class="upload-area" 
         :class="{ 'drag-over': isDragging }"
         @dragover.prevent="isDragging = true"
         @dragleave.prevent="isDragging = false"
         @drop.prevent="handleFileDrop">
      
      <div v-if="!uploading">
        <input type="file" 
               ref="fileInput" 
               accept=".pdf" 
               style="display: none"
               @change="handleFileSelect" />
        
        <button @click="$refs.fileInput.click()" class="upload-btn">
          选择PDF文件
        </button>
        
        <p>或将文件拖放到此处</p>
      </div>
      
      <div v-else class="progress">
        <div class="progress-bar" :style="{ width: uploadProgress + '%' }"></div>
        <p>上传中... {{ uploadProgress }}%</p>
      </div>
    </div>
    
    <div v-if="error" class="error-message">
      {{ error }}
    </div>
    
    <div v-if="processingDocuments.length > 0" class="processing-list">
      <h3>正在处理的文档</h3>
      <div v-for="doc in processingDocuments" :key="doc.id" class="processing-item">
        <p>{{ doc.filename }}</p>
        <p class="status">{{ getStatusText(doc.status) }}</p>
      </div>
    </div>
  </div>
</template>

<script>
import axios from 'axios';

export default {
  name: 'DocumentUpload',
  data() {
    return {
      isDragging: false,
      uploading: false,
      uploadProgress: 0,
      error: null,
      processingDocuments: []
    };
  },
  mounted() {
    this.fetchProcessingDocuments();
    // 定期检查处理状态
    this.statusInterval = setInterval(this.checkProcessingStatus, 5000);
  },
  beforeDestroy() {
    clearInterval(this.statusInterval);
  },
  methods: {
    handleFileSelect(event) {
      const file = event.target.files[0];
      if (file) {
        this.uploadFile(file);
      }
    },
    handleFileDrop(event) {
      this.isDragging = false;
      const file = event.dataTransfer.files[0];
      if (file && file.type === 'application/pdf') {
        this.uploadFile(file);
      } else {
        this.error = '请上传PDF文件';
      }
    },
    async uploadFile(file) {
      this.error = null;
      this.uploading = true;
      this.uploadProgress = 0;
      
      const formData = new FormData();
      formData.append('file', file);
      
      try {
        const response = await axios.post('/api/documents/upload', formData, {
          onUploadProgress: (progressEvent) => {
            this.uploadProgress = Math.round(
              (progressEvent.loaded * 100) / progressEvent.total
            );
          }
        });
        
        // 添加到处理列表
        this.processingDocuments.push({
          id: response.data.document_id,
          filename: file.name,
          status: 'processing'
        });
        
        // 重置上传状态
        this.uploading = false;
        this.$emit('upload-complete', response.data);
        
      } catch (error) {
        this.error = error.response?.data?.detail || '上传失败，请重试';
        this.uploading = false;
      }
    },
    async fetchProcessingDocuments() {
      try {
        const response = await axios.get('/api/documents');
        this.processingDocuments = response.data.filter(
          doc => doc.status === 'processing'
        );
      } catch (error) {
        console.error('获取处理中文档失败', error);
      }
    },
    async checkProcessingStatus() {
      const updatedDocs = [];
      
      for (const doc of this.processingDocuments) {
        try {
          const response = await axios.get(`/api/documents/${doc.id}`);
          if (response.data.status !== 'processing') {
            // 通知状态变化
            this.$emit('document-processed', response.data);
          } else {
            updatedDocs.push({
              ...doc,
              status: response.data.status
            });
          }
        } catch (error) {
          console.error(`检查文档 ${doc.id} 状态失败`, error);
          updatedDocs.push(doc);
        }
      }
      
      this.processingDocuments = updatedDocs;
    },
    getStatusText(status) {
      const statusMap = {
        'processing': '处理中...',
        'processed': '处理完成',
        'failed': '处理失败'
      };
      return statusMap[status] || status;
    }
  }
};
</script>

<style scoped>
.document-upload {
  margin-bottom: 2rem;
}

.upload-area {
  border: 2px dashed #ccc;
  border-radius: 8px;
  padding: 2rem;
  text-align: center;
  margin: 1rem 0;
  transition: all 0.3s;
}

.drag-over {
  border-color: #4a9eff;
  background-color: rgba(74, 158, 255, 0.1);
}

.upload-btn {
  padding: 0.75rem 1.5rem;
  background-color: #4a9eff;
  color: white;
  border: none;
  border-radius: 4px;
  cursor: pointer;
  font-size: 1rem;
  margin-bottom: 1rem;
}

.progress {
  width: 100%;
  height: 20px;
  background-color: #f0f0f0;
  border-radius: 4px;
  overflow: hidden;
  margin-bottom: 0.5rem;
}

.progress-bar {
  height: 100%;
  background-color: #4a9eff;
  transition: width 0.3s;
}

.error-message {
  color: #ff4a4a;
  margin-top: 1rem;
}

.processing-list {
  margin-top: 2rem;
}

.processing-item {
  display: flex;
  justify-content: space-between;
  padding: 0.5rem;
  border-bottom: 1px solid #eee;
}

.status {
  color: #777;
}
</style>
```

### 4.2 文档查询组件 (DocumentQuery.vue)

```vue
<template>
  <div class="document-query">
    <div v-if="!document">
      <p>请先选择一个文档</p>
    </div>
    
    <div v-else>
      <div class="document-info">
        <h2>{{ document.filename }}</h2>
        <div class="metadata">
          <p v-if="document.metadata.title"><strong>标题:</strong> {{ document.metadata.title }}</p>
          <p v-if="document.metadata.author"><strong>作者:</strong> {{ document.metadata.author }}</p>
          <p><strong>页数:</strong> {{ document.metadata.total_pages }}</p>
          <p><strong>文本块:</strong> {{ document.chunk_count }}</p>
        </div>
      </div>
      
      <div class="query-section">
        <h3>提问</h3>
        <textarea 
          v-model="question" 
          placeholder="请输入您关于文献的问题..." 
          rows="3"
          class="question-input"
        ></textarea>
        
        <div class="query-actions">
          <button 
            @click="submitQuestion" 
            :disabled="!question.trim() || loading"
            class="submit-btn"
          >
            {{ loading ? '思考中...' : '提交问题' }}
          </button>
        </div>
      </div>
      
      <div v-if="answer" class="answer-section">
        <h3>回答</h3>
        <div class="answer-content" v-html="formattedAnswer"></div>
        
        <div v-if="answer.sources && answer.sources.length" class="sources-section">
          <h4>参考来源</h4>
          <div class="sources-list">
            <div 
              v-for="(source, index) in answer.sources" 
              :key="index"
              class="source-item"
              @click="showSourceDetail(source)"
            >
              <div class="source-header">
                <span class="source-page">页码: {{ source.page }}</span>
                <span class="source-relevance">相关度: {{ (source.relevance * 100).toFixed(1) }}%</span>
              </div>
              <div class="source-preview">{{ source.preview }}</div>
            </div>
          </div>
        </div>
      </div>
      
      <div v-if="error" class="error-message">
        {{ error }}
      </div>
    </div>
    
    <!-- 来源详情对话框 -->
    <div v-if="showSourceModal" class="source-modal">
      <div class="modal-content">
        <div class="modal-header">
          <h3>参考内容详情</h3>
          <button @click="showSourceModal = false" class="close-btn">&times;</button>
        </div>
        <div class="modal-body">
          <p><strong>页码:</strong> {{ selectedSource.page }}</p>
          <p><strong>相关度:</strong> {{ (selectedSource.relevance * 100).toFixed(1) }}%</p>
          <div class="source-full-content">{{ selectedSource.preview }}</div>
        </div>
      </div>
    </div>
  </div>
</template>

<script>
import axios from 'axios';

export default {
  name: 'DocumentQuery',
  props: {
    document: {
      type: Object,
      default: null
    }
  },
  data() {
    return {
      question: '',
      answer: null,
      loading: false,
      error: null,
      showSourceModal: false,
      selectedSource: null
    };
  },
  computed: {
    formattedAnswer() {
      if (!this.answer || !this.answer.answer) return '';
      return this.answer.answer.replace(/\n/g, '<br>');
    }
  },
  methods: {
    async submitQuestion() {
      if (!this.question.trim() || !this.document) return;
      
      this.loading = true;
      this.error = null;
      this.answer = null;
      
      try {
        const response = await axios.post(`/api/documents/${this.document.id}/query`, {
          document_id: this.document.id,
          question: this.question.trim(),
          top_k: 5
        });
        
        this.answer = response.data;
      } catch (error) {
        this.error = error.response?.data?.detail || '查询失败，请重试';
      } finally {
        this.loading = false;
      }
    },
    showSourceDetail(source) {
      this.selectedSource = source;
      this.showSourceModal = true;
    }
  }
};
</script>

<style scoped>
.document-info {
  background-color: #f8f9fa;
  padding: 1rem;
  border-radius: 8px;
  margin-bottom: 2rem;
}

.metadata p {
  margin: 0.5rem 0;
}

.query-section {
  margin-bottom: 2rem;
}

.question-input {
  width: 100%;
  padding: 0.75rem;
  border: 1px solid #ddd;
  border-radius: 4px;
  font-size: 1rem;
  resize: vertical;
  margin-bottom: 1rem;
}

.submit-btn {
  padding: 0.75rem 1.5rem;
  background-color: #28a745;
  color: white;
  border: none;
  border-radius: 4px;
  cursor: pointer;
  font-size: 1rem;
}

.submit-btn:disabled {
  background-color: #6c757d;
  cursor: not-allowed;
}

.answer-section {
  background-color: #f8f9fa;
  padding: 1.5rem;
  border-radius: 8px;
  margin-bottom: 2rem;
}

.answer-content {
  line-height: 1.6;
  margin-bottom: 1rem;
}

.sources-section {
  margin-top: 2rem;
}

.sources-list {
  margin-top: 1rem;
}

.source-item {
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 4px;
  padding: 1rem;
  margin-bottom: 0.5rem;
  cursor: pointer;
  transition: all 0.3s;
}

.source-item:hover {
  border-color: #4a9eff;
  box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}

.source-header {
  display: flex;
  justify-content: space-between;
  margin-bottom: 0.5rem;
}

.source-page, .source-relevance {
  font-size: 0.9rem;
  color: #666;
}

.source-preview {
  font-size: 0.9rem;
  color: #333;
}

.error-message {
  color: #dc3545;
  padding: 1rem;
  background-color: #f8d7da;
  border-radius: 4px;
}

.source-modal {
  position: fixed;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background-color: rgba(0, 0, 0, 0.5);
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 1000;
}

.modal-content {
  background-color: white;
  border-radius: 8px;
  width: 90%;
  max-width: 600px;
  max-height: 80%;
  overflow-y: auto;
}

.modal-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 1rem;
  border-bottom: 1px solid #ddd;
}

.close-btn {
  background: none;
  border: none;
  font-size: 1.5rem;
  cursor: pointer;
}

.modal-body {
  padding: 1rem;
}

.source-full-content {
  background-color: #f8f9fa;
  padding: 1rem;
  border-radius: 4px;
  white-space: pre-wrap;
  margin-top: 1rem;
}
</style>
```

### 4.3 主应用组件 (App.vue)

```vue
<template>
  <div id="app">
    <header class="app-header">
      <h1>文献分析智能体</h1>
    </header>
    
    <main class="app-main">
      <div class="container">
        <div class="left-panel">
          <DocumentUpload 
            @upload-complete="handleUploadComplete"
            @document-processed="handleDocumentProcessed"
          />
          
          <DocumentList 
            :documents="documents"
            :selected-document="selectedDocument"
            @document-selected="handleDocumentSelect"
            @document-deleted="handleDocumentDeleted"
          />
        </div>
        
        <div class="right-panel">
          <DocumentQuery :document="selectedDocument" />
        </div>
      </div>
    </main>
  </div>
</template>

<script>
import DocumentUpload from './components/DocumentUpload.vue';
import DocumentList from './components/DocumentList.vue';
import DocumentQuery from './components/DocumentQuery.vue';
import axios from 'axios';

export default {
  name: 'App',
  components: {
    DocumentUpload,
    DocumentList,
    DocumentQuery
  },
  data() {
    return {
      documents: [],
      selectedDocument: null
    };
  },
  mounted() {
    this.fetchDocuments();
  },
  methods: {
    async fetchDocuments() {
      try {
        const response = await axios.get('/api/documents');
        this.documents = response.data;
      } catch (error) {
        console.error('获取文档列表失败', error);
      }
    },
    handleUploadComplete(uploadResponse) {
      // 处理上传完成，但可能还在处理中
      this.fetchDocuments();
    },
    handleDocumentProcessed(document) {
      // 文档处理完成
      this.fetchDocuments();
      if (document.status === 'processed') {
        this.$notify({
          title: '处理完成',
          message: `文档 ${document.filename} 处理完成，可以开始提问了！`,
          type: 'success'
        });
      } else if (document.status === 'failed') {
        this.$notify({
          title: '处理失败',
          message: `文档 ${document.filename} 处理失败：${document.error}`,
          type: 'error'
        });
      }
    },
    handleDocumentSelect(document) {
      if (document.status !== 'processed') {
        this.$notify({
          title: '文档未就绪',
          message: '文档还在处理中，请稍后再试',
          type: 'warning'
        });
        return;
      }
      this.selectedDocument = document;
    },
    handleDocumentDeleted() {
      this.fetchDocuments();
      this.selectedDocument = null;
    }
  }
};
</script>

<style>
* {
  box-sizing: border-box;
  margin: 0;
  padding: 0;
}

body {
  font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
  line-height: 1.6;
  color: #333;
}

.app-header {
  background-color: #2c3e50;
  color: white;
  padding: 1rem 0;
  text-align: center;
}

.app-main {
  padding: 2rem 0;
}

.container {
  max-width: 1200px;
  margin: 0 auto;
  padding: 0 1rem;
  display: grid;
  grid-template-columns: 1fr 1fr;
  gap: 2rem;
}

.left-panel, .right-panel {
  background-color: white;
  border-radius: 8px;
  box-shadow: 0 2px 10px rgba(0,0,0,0.1);
  padding: 1.5rem;
}

@media (max-width: 768px) {
  .container {
    grid-template-columns: 1fr;
  }
}
</style>
```

## 5. 配置文件

### 5.1 环境配置 (.env)

```env
# OpenAI配置
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-3.5-turbo

# 数据库配置
DATABASE_URL=postgresql://user:password@localhost:5432/document_analysis

# Redis配置（用于Celery）
REDIS_URL=redis://localhost:6379/0

# 文件上传配置
MAX_FILE_SIZE=50MB
UPLOAD_DIRECTORY=./uploads
VECTOR_DB_DIRECTORY=./vector_db

# API配置
API_HOST=0.0.0.0
API_PORT=8000
```

### 5.2 依赖配置 (requirements.txt)

```txt
# 后端框架
fastapi==0.104.1
uvicorn[standard]==0.24.0
python-multipart==0.0.6

# 数据库
sqlalchemy==2.0.23
psycopg2-binary==2.9.9
alembic==1.12.1

# 任务队列
celery==5.3.4
redis==5.0.1

# LangChain相关
langchain==0.0.335
langchain-community==0.0.6
langchain-openai==0.0.2
chromadb==0.4.17
openai==1.3.5

# 文档处理
PyMuPDF==1.23.8
pypdf==3.17.1
python-docx==1.1.0

# 工具库
pydantic==2.5.0
python-dotenv==1.0.0
httpx==0.25.2
numpy==1.24.3
tiktoken==0.5.1

# 开发工具
pytest==7.4.3
pytest-asyncio==0.21.1
black==23.11.0
```

### 5.3 Docker配置

#### Dockerfile

```dockerfile
FROM python:3.10-slim

WORKDIR /app

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# 复制依赖文件
COPY requirements.txt .

# 安装Python依赖
RUN pip install --no-cache-dir -r requirements.txt

# 复制源代码
COPY . .

# 创建必要目录
RUN mkdir -p uploads vector_db logs

# 暴露端口
EXPOSE 8000

# 启动命令
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### docker-compose.yml

```yaml
version: '3.8'

services:
  # 主API服务
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://postgres:password@postgres:5432/document_analysis
      - REDIS_URL=redis://redis:6379/0
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./uploads:/app/uploads
      - ./vector_db:/app/vector_db
      - ./logs:/app/logs
    depends_on:
      - postgres
      - redis
    restart: unless-stopped

  # Celery工作者
  worker:
    build: .
    command: celery -A app.celery worker --loglevel=info
    environment:
      - DATABASE_URL=postgresql://postgres:password@postgres:5432/document_analysis
      - REDIS_URL=redis://redis:6379/0
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./uploads:/app/uploads
      - ./vector_db:/app/vector_db
      - ./logs:/app/logs
    depends_on:
      - postgres
      - redis
    restart: unless-stopped

  # PostgreSQL数据库
  postgres:
    image: postgres:15
    environment:
      - POSTGRES_DB=document_analysis
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    restart: unless-stopped

  # Redis
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    restart: unless-stopped

  # 前端（可选）
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:80"
    depends_on:
      - api
    restart: unless-stopped

volumes:
  postgres_data:
```

## 6. 测试和优化

### 6.1 单元测试 (tests/test_agent.py)

```python
import pytest
import tempfile
import os
from unittest.mock import Mock, patch

from app.document_processor import DocumentProcessor
from app.vector_store import VectorStoreManager
from app.agent_core import DocumentAnalysisAgent

class TestDocumentProcessor:
    def setup_method(self):
        self.processor = DocumentProcessor()
    
    def test_pdf_extraction(self):
        # 测试PDF文本提取
        # 这里需要准备一个测试PDF文件
        pass
    
    def test_text_splitting(self):
        test_text = "这是一个测试文档。" * 100
        docs = self.processor.split_text(test_text, {"title": "测试"})
        assert len(docs) > 0
        assert all(doc.metadata["title"] == "测试" for doc in docs)

class TestVectorStore:
    def setup_method(self):
        self.temp_dir = tempfile.mkdtemp()
        self.vector_store = VectorStoreManager(persist_directory=self.temp_dir)
    
    @patch('langchain.embeddings.OpenAIEmbeddings')
    def test_create_vector_store(self, mock_embeddings):
        # 测试向量存储创建
        pass

class TestAgent:
    def setup_method(self):
        self.agent = DocumentAnalysisAgent()
    
    @patch('app.vector_store.VectorStoreManager')
    @patch('langchain.chat_models.ChatOpenAI')
    def test_answer_question(self, mock_llm, mock_vector_store):
        # 模拟向量存储返回结果
        mock_vector_store.search_documents.return_value = [
            {
                "content": "这是测试内容",
                "metadata": {"page": 1, "chunk_id": 0},
                "relevance_score": 0.95
            }
        ]
        
        # 模拟LLM回答
        mock_llm.return_value.predict.return_value = "这是测试回答"
        
        result = self.agent.answer_question("test_doc", "测试问题")
        
        assert "answer" in result
        assert "sources" in result
        assert len(result["sources"]) > 0
```

### 6.2 性能优化 (optimizations.py)

```python
import asyncio
import concurrent.futures
import re
import uuid
from typing import List, Dict, Any
from langchain.schema import Document

class OptimizedDocumentProcessor(DocumentProcessor):
    """优化的文档处理器，支持大文档和多线程处理"""
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200, max_workers: int = 4):
        super().__init__(chunk_size, chunk_overlap)
        self.max_workers = max_workers
    
    async def process_large_document(self, file_path: str) -> Dict[str, Any]:
        """异步处理大型文档"""
        try:
            # 使用线程池处理CPU密集型任务
            loop = asyncio.get_event_loop()
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # 在线程池中运行PDF处理
                text, metadata = await loop.run_in_executor(
                    executor, self.extract_text_from_pdf, file_path
                )
                
                # 并行分割文本
                docs = await loop.run_in_executor(
                    executor, self.split_text, text, metadata
                )
            
            return {
                "document_id": str(uuid.uuid4()),
                "chunks": docs,
                "metadata": metadata,
                "status": "processed",
                "chunk_count": len(docs)
            }
        except Exception as e:
            return {
                "document_id": str(uuid.uuid4()),
                "status": "failed",
                "error": str(e)
            }

class OptimizedVectorStore(VectorStoreManager):
    """优化的向量存储，支持缓存和混合检索"""
    
    def __init__(self, embedding_model=None, persist_directory="./vector_db", cache_size=1000):
        super().__init__(embedding_model, persist_directory)
        self.cache = {}
        self.cache_size = cache_size
    
    def search_with_cache(self, document_id: str, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """带缓存的搜索"""
        cache_key = f"{document_id}:{query}:{top_k}"
        
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        results = self.search_documents(document_id, query, top_k)
        
        # 简单LRU缓存策略
        if len(self.cache) >= self.cache_size:
            # 删除最老的缓存项
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
        
        self.cache[cache_key] = results
        return results
    
    def hybrid_search(self, document_id: str, query: str, top_k: int = 5, 
                     alpha: float = 0.7) -> List[Dict[str, Any]]:
        """混合检索：向量搜索 + 关键词搜索"""
        # 向量搜索
        vector_results = self.search_documents(document_id, query, top_k * 2)
        
        # 关键词搜索（简单实现）
        keyword_results = self._keyword_search(document_id, query, top_k * 2)
        
        # 融合结果
        combined_results = self._combine_search_results(
            vector_results, keyword_results, alpha
        )
        
        return combined_results[:top_k]
    
    def _keyword_search(self, document_id: str, query: str, top_k: int) -> List[Dict[str, Any]]:
        """关键词搜索实现"""
        vector_store = self.load_vector_store(document_id)
        
        # 获取所有文档
        all_docs = vector_store._collection.get()
        
        # 简单的关键词匹配
        results = []
        for i, content in enumerate(all_docs['documents']):
            score = self._calculate_keyword_score(content, query)
            if score > 0:
                results.append({
                    "content": content,
                    "metadata": all_docs['metadatas'][i],
                    "relevance_score": score
                })
        
        # 按分数排序
        results.sort(key=lambda x: x['relevance_score'], reverse=True)
        return results[:top_k]
    
    def _calculate_keyword_score(self, content: str, query: str) -> float:
        """计算关键词匹配分数"""
        import jieba
        
        # 分词
        query_words = set(jieba.cut(query.lower()))
        content_words = set(jieba.cut(content.lower()))
        
        # 计算交集比例
        intersection = query_words & content_words
        union = query_words | content_words
        
        if len(union) == 0:
            return 0.0
        
        return len(intersection) / len(union)
    
    def _combine_search_results(self, vector_results: List[Dict], 
                              keyword_results: List[Dict], alpha: float) -> List[Dict]:
        """融合搜索结果"""
        # 归一化分数
        def normalize_scores(results):
            if not results:
                return results
            
            max_score = max(r['relevance_score'] for r in results)
            min_score = min(r['relevance_score'] for r in results)
            
            if max_score == min_score:
                return results
            
            for r in results:
                r['relevance_score'] = (r['relevance_score'] - min_score) / (max_score - min_score)
            
            return results
        
        vector_results = normalize_scores(vector_results)
        keyword_results = normalize_scores(keyword_results)
        
        # 创建内容到结果的映射
        content_map = {}
        
        # 添加向量搜索结果
        for result in vector_results:
            content = result['content']
            content_map[content] = {
                **result,
                'vector_score': result['relevance_score'],
                'keyword_score': 0.0
            }
        
        # 添加关键词搜索结果
        for result in keyword_results:
            content = result['content']
            if content in content_map:
                content_map[content]['keyword_score'] = result['relevance_score']
            else:
                content_map[content] = {
                    **result,
                    'vector_score': 0.0,
                    'keyword_score': result['relevance_score']
                }
        
        # 计算融合分数
        for content, result in content_map.items():
            combined_score = (alpha * result['vector_score'] + 
                            (1 - alpha) * result['keyword_score'])
            result['relevance_score'] = combined_score
        
        # 排序并返回
        final_results = list(content_map.values())
        final_results.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        return final_results
```

## 7. 部署指南

### 7.1 本地开发环境

```bash
# 1. 克隆项目
git clone <your-repo-url>
cd document-analysis-agent

# 2. 创建虚拟环境
python -m venv venv
source venv/bin/activate  # Linux/Mac
# 或 venv\Scripts\activate  # Windows

# 3. 安装依赖
pip install -r requirements.txt

# 4. 配置环境变量
cp .env.example .env
# 编辑 .env 文件，填入你的配置

# 5. 初始化数据库
alembic upgrade head

# 6. 启动服务
uvicorn app:app --reload --host 0.0.0.0 --port 8000

# 7. 启动Celery工作者（新终端）
celery -A app.celery worker --loglevel=info

# 8. 启动前端（可选）
cd frontend
npm install
npm run serve
```

### 7.2 生产环境部署

```bash
# 1. 使用Docker Compose部署
git clone <your-repo-url>
cd document-analysis-agent

# 2. 配置环境变量
cp .env.example .env
# 编辑 .env 文件，配置生产环境变量

# 3. 启动服务
docker-compose up -d

# 4. 初始化数据库
docker-compose exec api alembic upgrade head

# 5. 查看日志
docker-compose logs -f
```

### 7.3 Nginx配置（可选）

```nginx
server {
    listen 80;
    server_name your-domain.com;

    # API代理
    location /api/ {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # 文件上传大小限制
        client_max_body_size 100M;
    }

    # 前端静态文件
    location / {
        proxy_pass http://localhost:3000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

## 8. 核心功能总结

### 8.1 主要功能

1. **文档上传与处理**
   - 支持PDF文件上传
   - 自动文本提取和分块
   - 异步处理，支持大文件

2. **向量化存储**
   - 使用OpenAI Embeddings进行文本嵌入
   - ChromaDB持久化存储
   - 支持多文档并行管理

3. **智能问答**
   - 基于文档内容的精确回答
   - 提供答案来源和相关度
   - 支持多轮对话上下文

4. **用户界面**
   - 现代化Vue.js前端界面
   - 拖拽上传功能
   - 实时处理状态显示

### 8.2 技术特色

1. **模块化设计**
   - 清晰的组件分离
   - 易于扩展和维护
   - 支持插件化开发

2. **性能优化**
   - 异步处理架构
   - 缓存机制
   - 混合检索策略

3. **错误处理**
   - 完善的异常捕获
   - 用户友好的错误提示
   - 自动重试机制

4. **生产就绪**
   - Docker容器化部署
   - 数据库迁移管理
   - 监控和日志记录

## 9. 扩展建议

### 9.1 功能扩展

1. **多模态支持**
   - 图片OCR识别
   - 表格结构化提取
   - 音频转文字

2. **批量处理**
   - 多文件同时上传
   - 批量问答API
   - 导出功能

3. **高级检索**
   - 语义搜索优化
   - 实体关系提取
   - 知识图谱构建

4. **用户管理**
   - 多用户支持
   - 权限控制
   - 使用统计

### 9.2 API增强

1. **RESTful API完善**
   - 分页查询
   - 高级过滤
   - 批量操作

2. **WebSocket支持**
   - 实时处理状态
   - 流式问答
   - 实时通知

3. **API文档**
   - OpenAPI/Swagger集成
   - 交互式API文档
   - SDK生成

### 9.3 数据库模型补充 (database.py)

```python
from sqlalchemy import create_engine, Column, String, DateTime, Integer, Text, JSON, Boolean
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from sqlalchemy.dialects.postgresql import UUID
import uuid
from datetime import datetime

Base = declarative_base()

class Document(Base):
    """文档数据模型"""
    __tablename__ = "documents"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    filename = Column(String(255), nullable=False)
    file_path = Column(String(500), nullable=False)
    status = Column(String(50), nullable=False, default="processing")
    metadata = Column(JSON, nullable=True)
    vector_store_path = Column(String(500), nullable=True)
    chunk_count = Column(Integer, nullable=True)
    error = Column(Text, nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

class QueryHistory(Base):
    """查询历史记录"""
    __tablename__ = "query_history"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    document_id = Column(UUID(as_uuid=True), nullable=False)
    question = Column(Text, nullable=False)
    answer = Column(Text, nullable=False)
    sources = Column(JSON, nullable=True)
    processing_time = Column(Integer, nullable=True)  # 毫秒
    created_at = Column(DateTime, default=datetime.utcnow)

# 数据库连接配置
def create_database_engine(database_url: str):
    """创建数据库引擎"""
    engine = create_engine(database_url)
    return engine

def get_session_local(engine):
    """创建会话工厂"""
    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    return SessionLocal

def get_db():
    """依赖注入：获取数据库会话"""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

### 9.4 Celery任务配置 (celery_app.py)

```python
from celery import Celery
import os
from .document_processor import DocumentProcessor
from .vector_store import VectorStoreManager
from .database import get_session_local, create_database_engine

# 创建Celery应用
celery_app = Celery(
    "document_analysis",
    broker=os.getenv("REDIS_URL", "redis://localhost:6379/0"),
    backend=os.getenv("REDIS_URL", "redis://localhost:6379/0")
)

# 配置Celery
celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
    task_track_started=True,
    task_time_limit=30 * 60,  # 30分钟超时
    worker_prefetch_multiplier=1,
    worker_max_tasks_per_child=50,
)

# 初始化组件
document_processor = DocumentProcessor()
vector_store_manager = VectorStoreManager()

@celery_app.task(bind=True)
def process_document_task(self, document_id: str, file_path: str):
    """异步处理文档任务"""
    try:
        # 更新任务状态
        self.update_state(state="PROCESSING", meta={"step": "extracting_text"})
        
        # 处理文档
        result = document_processor.process_document(file_path)
        
        if result["status"] == "failed":
            self.update_state(state="FAILURE", meta={"error": result["error"]})
            return {"status": "failed", "error": result["error"]}
        
        # 更新状态：创建向量存储
        self.update_state(state="PROCESSING", meta={"step": "creating_vectors"})
        
        # 创建向量存储
        vector_store_path = vector_store_manager.create_vector_store(
            document_id=document_id,
            chunks=result["chunks"]
        )
        
        # 更新数据库
        engine = create_database_engine(os.getenv("DATABASE_URL"))
        SessionLocal = get_session_local(engine)
        db = SessionLocal()
        
        try:
            db.query(Document).filter(Document.id == document_id).update({
                "status": "processed",
                "metadata": result["metadata"],
                "vector_store_path": vector_store_path,
                "chunk_count": result["chunk_count"],
                "updated_at": datetime.utcnow()
            })
            db.commit()
        finally:
            db.close()
        
        return {
            "status": "processed",
            "chunk_count": result["chunk_count"],
            "vector_store_path": vector_store_path
        }
        
    except Exception as e:
        # 更新失败状态
        self.update_state(state="FAILURE", meta={"error": str(e)})
        
        # 更新数据库
        engine = create_database_engine(os.getenv("DATABASE_URL"))
        SessionLocal = get_session_local(engine)
        db = SessionLocal()
        
        try:
            db.query(Document).filter(Document.id == document_id).update({
                "status": "failed",
                "error": str(e),
                "updated_at": datetime.utcnow()
            })
            db.commit()
        finally:
            db.close()
        
        return {"status": "failed", "error": str(e)}
```

### 9.5 监控和日志配置 (logging_config.py)

```python
import logging
import logging.handlers
import os
from datetime import datetime

def setup_logging():
    """配置应用日志"""
    
    # 创建日志目录
    log_dir = "./logs"
    os.makedirs(log_dir, exist_ok=True)
    
    # 配置格式
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # 根日志器
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    
    # 控制台处理器
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)
    
    # 文件处理器（按时间轮转）
    file_handler = logging.handlers.TimedRotatingFileHandler(
        filename=os.path.join(log_dir, "app.log"),
        when="midnight",
        interval=1,
        backupCount=30,
        encoding="utf-8"
    )
    file_handler.setFormatter(formatter)
    root_logger.addHandler(file_handler)
    
    # 错误日志单独记录
    error_handler = logging.handlers.TimedRotatingFileHandler(
        filename=os.path.join(log_dir, "error.log"),
        when="midnight",
        interval=1,
        backupCount=30,
        encoding="utf-8"
    )
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(formatter)
    root_logger.addHandler(error_handler)
    
    # 设置特定模块的日志级别
    logging.getLogger("uvicorn").setLevel(logging.INFO)
    logging.getLogger("celery").setLevel(logging.INFO)
    logging.getLogger("sqlalchemy.engine").setLevel(logging.WARNING)
    
    return root_logger

# 应用启动时调用
logger = setup_logging()
```

这个完整的实现方案提供了一个生产级别的PDF文献分析智能体系统，具有良好的可扩展性、性能优化和错误处理能力。所有组件都经过精心设计，可以独立测试和部署，适合在实际项目中使用。